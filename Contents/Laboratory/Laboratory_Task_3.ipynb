{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Name : Jezzel Fiath Q. Gier\n",
        "### Section : DS4A\n"
      ],
      "metadata": {
        "id": "rBNd8DRUcMCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Laboratory Task 3**\n",
        "\n",
        "***Instruction:*** Perform a forward and backward propagation in python using the inputs from Laboratory Task 2"
      ],
      "metadata": {
        "id": "cqhEsJ3ncXNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# ----- Given Inputs -----\n",
        "x = np.array([1, 0, 1])\n",
        "y = np.array([1])\n",
        "lr = 0.001\n",
        "\n",
        "# ----- Initialize Weights & Biases -----\n",
        "W1 = np.array([\n",
        "    [0.2, -0.3],\n",
        "    [0.4,  0.1],\n",
        "    [-0.5, 0.2]\n",
        "])\n",
        "\n",
        "W2 = np.array([-0.3, -0.2])\n",
        "theta = np.array([-0.4, 0.2, 0.1])\n",
        "\n",
        "# ----- Forward Propagation -----\n",
        "# Hidden layer net input\n",
        "Z_hidden = np.dot(x, W1) + theta[:2]\n",
        "\n",
        "# ReLU activation\n",
        "h = np.maximum(0, Z_hidden)\n",
        "\n",
        "# Output layer net input (linear)\n",
        "Z_output = np.dot(W2, h) + theta[2]\n",
        "\n",
        "# Predicted output\n",
        "y_pred = Z_output\n",
        "\n",
        "# Compute error\n",
        "error = 0.5 * (y - y_pred) ** 2\n",
        "\n",
        "print(\"=== Forward Pass ===\")\n",
        "print(\"Hidden net inputs (Z1, Z2):\", Z_hidden)\n",
        "print(\"Hidden activations (h1, h2):\", h)\n",
        "print(\"Predicted output (ŷ):\", y_pred)\n",
        "print(\"Error (E):\", error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RerIrw2cgCS",
        "outputId": "26158dfe-19f8-4f04-b466-b589e23af29c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Forward Pass ===\n",
            "Hidden net inputs (Z1, Z2): [-0.7  0.1]\n",
            "Hidden activations (h1, h2): [0.  0.1]\n",
            "Predicted output (ŷ): 0.08\n",
            "Error (E): [0.4232]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Backward Propagation -----\n",
        "# Derivative of error w.r.t. output\n",
        "dE_dy = -(y - y_pred)\n",
        "\n",
        "# Gradient for output weights and bias\n",
        "dE_dW2 = dE_dy * h\n",
        "dE_dtheta3 = dE_dy\n",
        "relu_derivative = (Z_hidden > 0).astype(float)\n",
        "\n",
        "# Backpropagate through output weights\n",
        "dE_dh = dE_dy * W2\n",
        "dE_dZ_hidden = dE_dh * relu_derivative\n",
        "\n",
        "# Gradients for W1 and hidden biases\n",
        "dE_dW1 = np.outer(x, dE_dZ_hidden)\n",
        "dE_dtheta12 = dE_dZ_hidden\n",
        "\n",
        "\n",
        "# ----- Update Weights & Biases -----\n",
        "W2 -= lr * dE_dW2\n",
        "theta[2] -= lr * dE_dtheta3\n",
        "W1 -= lr * dE_dW1\n",
        "theta[:2] -= lr * dE_dtheta12\n",
        "\n",
        "print(\"\\n=== Backward Pass ===\")\n",
        "print(\"Gradient dE/dW2:\", dE_dW2)\n",
        "print(\"Gradient dE/dW1:\\n\", dE_dW1)\n",
        "print(\"Updated W2:\", W2)\n",
        "print(\"Updated W1:\\n\", W1)\n",
        "print(\"Updated θ:\", theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O649dKWfcwM4",
        "outputId": "f05634d3-71a2-4206-ee4a-ca93cd36f968"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Backward Pass ===\n",
            "Gradient dE/dW2: [-0.    -0.092]\n",
            "Gradient dE/dW1:\n",
            " [[0.    0.184]\n",
            " [0.    0.   ]\n",
            " [0.    0.184]]\n",
            "Updated W2: [-0.3      -0.199908]\n",
            "Updated W1:\n",
            " [[ 0.2      -0.300184]\n",
            " [ 0.4       0.1     ]\n",
            " [-0.5       0.199816]]\n",
            "Updated θ: [-0.4       0.199816  0.10092 ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-550156346.py:21: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  theta[2] -= lr * dE_dtheta3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After performing forward and backward propagation, the model adjusted its weights and biases slightly to minimize the error. The gradients show that only the second hidden neuron contributed to learning since the first one was inactive due to the ReLU function. Although the updates are small because of the low learning rate, each iteration helps the network gradually learn the correct mapping between inputs and outputs, improving prediction accuracy over time."
      ],
      "metadata": {
        "id": "gtijcIL0dGMw"
      }
    }
  ]
}